{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://www.w-hs.de/maschinenbau-master-boh/\">\n",
    "    <img src=\"https://www.w-hs.de/typo3conf/ext/whs/Resources/Public/Images/Pagelayout/w-hs_pagelogo.png\" \n",
    "    alt=\"Images\" width=\"500\" height=\"200\">\n",
    "  </a>\n",
    "</div>\n",
    "<br>\n",
    "<h1 align=\"center\"> Sondergebiete der Simulation</h1>\n",
    "<h3 align=\"center\"> WS 21/22 </h3>\n",
    "<br />\n",
    "\n",
    "## Workflow\n",
    "\n",
    "\n",
    "> * Symbolische Herleitung der Bewegungsgleichung des freien, gedämpften Inversen Pendels mit **`sympy`**.\n",
    "> * Numerische Lösung der DGL mit Hilfe von **`scipy.itegrate.odeint`**.\n",
    "> * Trainingspunkte, die eine reale Messung ersetzen, aus numerischer Lösung extrahieren.\n",
    "> * Interpolation der Punkte mit Hilfe eines *Neuronales Netzes.\n",
    "> * Training eines PINNs, um den Bewegungsablauf ausßerhalb der Trainingspunkte anzunähern, indem DGL in die Loss-Funktion eingebettet wird.\n",
    "\n",
    "\n",
    "### Environment set up\n",
    "\n",
    "Sollten Fehlermeldungen erscheinen, obwohl alle benötigten Module installiert sind, hilft es wohlmöglich eine virtuelle Umgebung für das Arbeiten mit PINNs einzurichten:\n",
    "```bash\n",
    "conda create -n pinn python=3\n",
    "conda activate pinn\n",
    "conda install jupyter numpy matplotlib\n",
    "conda install pytorch torchvision torchaudio -c pytorch\n",
    "conda install scipy, sympy\n",
    "```\n",
    "\n",
    "#### Activate Enviroment\n",
    "```bash\n",
    "conda activate pinn\n",
    "jupyter notebook \n",
    "```\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "# Credits: [benmoseley](https://github.com/benmoseley/harmonic-oscillator-pinn)\n",
    "\n",
    "<br />\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy\n",
    "from sympy import symbols, Function, diff, sin, cos, Matrix, Rational, Eq, solve, lambdify\n",
    "import sympy.physics.mechanics as mech\n",
    "mech.init_vprinting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Herleitung der Bewegungsgleichungen \n",
    "\n",
    "Ausführliche Beschreibung der Herleitung:\n",
    "[Inverted Pendulum Legrange](https://github.com/lennart2810/InvertedPendulumSDS/blob/master/MKS/Inverted%20Pendulum%20Legrange.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbolischen Variablen anlegen\n",
    "t, l, M, m, g, D, d, F = symbols('t l M m g, D, d, F')\n",
    "\n",
    "# Zwangsbedingung\n",
    "y = 0\n",
    "\n",
    "# generalisierte Koordinaten und deren Ableitungen:\n",
    "x = Function('x')(t)\n",
    "x_d = diff(x,t)\n",
    "x_dd = diff(x_d,t)\n",
    "\n",
    "theta = Function('theta')(t)\n",
    "theta_d = diff(theta,t)\n",
    "theta_dd = diff(theta,t,t)\n",
    "\n",
    "# Kinematik\n",
    "x2, y2 = x - l * sin(theta), y + l * cos(theta)\n",
    "x2_d, y2_d = x2.diff(t), y2.diff(t)\n",
    "\n",
    "# Legrange-Funktion\n",
    "T = Rational(1,2) * M * (x_d)**2 + Rational(1,2) * m * (x2_d**2 + y2_d**2)\n",
    "V = M * g * y + m * g * y2 \n",
    "L = T - V\n",
    "\n",
    "# Euler-Legrange\n",
    "LE_x = diff(diff(L, x_d), t) - diff(L, x)\n",
    "LE_x = Eq(LE_x, F -  x_d * D)\n",
    "LE_theta = diff(diff(L, theta_d), t) - diff(L, theta)\n",
    "LE_theta = Eq(LE_theta, -theta_d * d)\n",
    "display(Matrix([[LE_x.simplify()], [LE_theta.simplify()]]))\n",
    "\n",
    "# Nach x_dd und theta_dd umstellen\n",
    "solutions = solve([LE_x, LE_theta], (x_dd, theta_dd), simplify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = D*x_d + M*x_dd + m*(l*sin(theta)*theta_d**2-l*cos(theta)*theta_dd+x_dd)\n",
    "\n",
    "# 0 = d*theta_d + l*m*(-g*sin(theta)+l*theta_dd-cos(theta)*x_dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umwandlung des symbolischen Gleichungssystems in numerische Funktionen mit `sympy.lambdify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxdt = lambdify(x_d, x_d)\n",
    "dvdt = lambdify((t,g,M,m,l,x_d,theta,theta_d,F,D,d), solutions[x_dd])\n",
    "\n",
    "dthetadt = lambdify(theta_d, theta_d)\n",
    "domegadt = lambdify((t,g,M,m,l,x_d,theta,theta_d,F,D,d), solutions[theta_dd])\n",
    "\n",
    "def dSdt(S, t, M, m, D, d, l, F):\n",
    "    _x, _v, _theta, _omega = S\n",
    "    return [\n",
    "        dxdt(_v),\n",
    "        dvdt(t, g, M, m, l, _v, _theta, _omega, F, D, d),\n",
    "        dthetadt(_omega),\n",
    "        domegadt(t, g, M, m , l, _v, _theta, _omega, F, D, d)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten generieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import odeint\n",
    "from scipy.interpolate import interpolate\n",
    "\n",
    "t1 = 10                          # s \n",
    "samples = 300                    # n \n",
    "t = np.linspace(0, t1, samples)  # s\n",
    "\n",
    "g = 9.81                         # m/s^2\n",
    "M = 5                            # kg\n",
    "m = 1                            # kg\n",
    "l = 1                            # m\n",
    "F = 0                            # N\n",
    "D = 0.5                          # N*s/m\n",
    "d = 0.1                          # Nm*s\n",
    "\n",
    "S0 = [0, 0, 0.1, 0]              # m, m/s, rad, rad/s\n",
    "\n",
    "# numerische Lösung\n",
    "ans = odeint(dSdt, y0=S0, t=t, args=(M, m, D, d, l, F))\n",
    "x = ans.T[0]\n",
    "theta = ans.T[2]\n",
    "\n",
    "# numerische Lösung --> Lösungsfunktion interpolieren (für DeepXDE)\n",
    "t_new = np.linspace(0, t1, samples+2000)    \n",
    "x_f = interpolate.interp1d(t, x, 'cubic')\n",
    "theta_f = interpolate.interp1d(t, theta, 'cubic')\n",
    "\n",
    "# 'Messwerte'\n",
    "a, b, i = 10, 180, 3 # jeden i. Punkt im Intervall [a,b] \n",
    "t_data = t[a:b:i]\n",
    "x_data = x[a:b:i]\n",
    "theta_data = theta[a:b:i]\n",
    "\n",
    "# Data für NN zusammenfassen: (nur nötig wenn 1 Model mit 2 Outputs genutzt wird)\n",
    "data = np.concatenate((x_data, theta_data)).reshape(2, len(t_data)).T\n",
    "print('time.shape:', t_data.shape, type(t_data))\n",
    "print('x.shape:', x_data.shape)\n",
    "print('theta.shape:', theta_data.shape)\n",
    "print('data.shape:', data.shape)\n",
    "\n",
    "\n",
    "# Visualisierung \n",
    "plt.plot(t, x, 'b.')\n",
    "plt.plot(t_new, x_f(t_new), 'g-')\n",
    "plt.plot(t_data, data[:,0], 'ro') # data[:,1] --> x_data\n",
    "plt.show()\n",
    "\n",
    "plt.plot(t, theta, 'r.')\n",
    "plt.plot(t_new, theta_f(t_new), 'y-')\n",
    "plt.plot(t_data, data[:,1], 'go') # data[:,1] --> theta_data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor(t).view(-1,1)\n",
    "t_data = torch.Tensor(t_data).view(-1,1)\n",
    "x_data = torch.Tensor(x_data).view(-1,1)\n",
    "theta_data = torch.Tensor(theta_data).view(-1,1)\n",
    "data = torch.Tensor(data).view(-1,2)\n",
    "\n",
    "print('t:', t.shape)\n",
    "print('t_data:', t_data.shape)\n",
    "print('x_data:', x_data.shape)\n",
    "print('theta_data:', theta_data.shape)\n",
    "print('data:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def save_gif_PIL(outfile, files, fps=5, loop=0):\n",
    "    \"Helper function for saving GIFs\"\n",
    "    imgs = [Image.open(file) for file in files]\n",
    "    imgs[0].save(fp=outfile, format='GIF', append_images=imgs[1:], save_all=True, duration=int(1000/fps), loop=loop)\n",
    "\n",
    "    \n",
    "def plot_my_result(t, x, theta, t_data, data, pred, ps=None):\n",
    "    \n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    fig.suptitle(name_plot, fontsize=10)\n",
    "    \n",
    "    # wahren Werte\n",
    "    plt.plot(t, x, 'k--', linewidth=2, alpha=0.8, label=\"numerical solution $x$\")\n",
    "    plt.plot(t, theta, 'b--', linewidth=2, alpha=0.8, label=r\"numerical solution $\\theta$\")\n",
    "    \n",
    "    # Messwerte\n",
    "    plt.scatter(t_data, data[:,0], s=30, color=\"grey\", label='Training data $x$')\n",
    "    plt.scatter(t_data, data[:,1], s=30, color=\"royalblue\", label=r'Training data $\\theta$')\n",
    "    \n",
    "    # Zeitpunkte der Physik Loss berechnung:\n",
    "    if ps is not None:\n",
    "        plt.scatter(ps, -3*torch.ones_like(ps), s=20, color=\"limegreen\", alpha=0.4, label='physics trainings')\n",
    "    \n",
    "    # Prediction\n",
    "    plt.plot(t, pred[:,0], color=\"black\", linewidth=2, alpha=0.8, label=\"Prediction $x$\")\n",
    "    plt.plot(t, pred[:,1], color=\"blue\", linewidth=2, alpha=0.8, label=r\"Prediction $\\theta$\")\n",
    "    \n",
    "    plt.text(torch.max(t)*1.01,torch.max(data),\"episode: %i\"%(i),fontsize=16,color=\"k\")\n",
    "    \n",
    "    \n",
    "    x_max = torch.max(t)\n",
    "    y_min = torch.min(pred[:,1])\n",
    "    y_max = torch.max(pred[:,1])\n",
    "    \n",
    "    plt.xlim(0, t1)\n",
    "    plt.ylim(-1, 6.3)\n",
    "    \n",
    "    l = plt.legend(loc=(1.01,0.34), frameon=False, fontsize=\"large\")\n",
    "    plt.setp(l.get_texts(), color=\"k\")\n",
    "    #ax.get_yaxis().set_ticks([])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classe eines Neuronalen Netzes erstellen, (mit einer forward Methode --> predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, N_INPUT, N_OUTPUT, N_HIDDEN, N_LAYERS):\n",
    "        super().__init__()\n",
    "        activation = nn.Tanh\n",
    "        self.fcs = nn.Sequential(*[\n",
    "                        nn.Linear(N_INPUT, N_HIDDEN),\n",
    "                        activation()])\n",
    "        self.fch = nn.Sequential(*[\n",
    "                        nn.Sequential(*[\n",
    "                            nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "                            activation()]) for _ in range(N_LAYERS-1)])\n",
    "        self.fce = nn.Linear(N_HIDDEN, N_OUTPUT)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fcs(x)\n",
    "        x = self.fch(x)\n",
    "        x = self.fce(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal neural network\n",
    "\n",
    "> Next, we train a standard neural network (fully connected) to fit these training points.\n",
    "\n",
    ">We find that the network is able to fit the solution very closely in the vicinity of the training points, but does not learn an accurate solution outside of them.\n",
    "\n",
    ">\"One popular way of doing this using machine learning is to use a neural network. Given the location of a data point as input (denoted x), a neural network can be used to output a prediction of its value (denoted u), as shown in the figure below.\" [Ben Moseley](https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/)\n",
    "\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/\">\n",
    "    <img src=\"https://benmoseley.blog/wp-content/uploads/2021/08/nn.png\" \n",
    "    alt=\"Images\" width=\"600\">\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "> To learn a model, we try to tune the network’s free parameters (denoted by the \\thetas in the figure above) so that the network’s predictions closely match the available experimental data. This is usually done by minimising the mean-squared-error between its predictions and the training points;\n",
    "\n",
    ">The problem is, using a purely data-driven approach like this can have significant downsides. Have a look at the actual values of the unknown physical process used to generate the experimental data in the animation above (grey line).\n",
    "\n",
    ">You can see that whilst the neural network accurately models the physical process within the vicinity of the experimental data, it fails to generalise away from this training data. By only relying on the data, one could argue it hasn’t truly “understood” the scientific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "lr_data = 1e-3\n",
    "num_layers = 3\n",
    "num_hidden = 32\n",
    "\n",
    "model = FCN(1,2,num_hidden,num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr_data)\n",
    "\n",
    "episodes = 6000\n",
    "num_save_plot = round(episodes/60) # übernimmt 60 Bilder ins gif\n",
    "\n",
    "files = []\n",
    "name_plot = 'Inverted Pendulum NN: lr %s, hidden %sx%s'%(lr_data, num_layers, num_hidden)\n",
    "name_gif = 'NN_lr_%s_hidden_%sx%s_episodes_%s.gif'%(lr_data, num_layers, num_hidden, episodes)\n",
    "\n",
    "for i in range(1, episodes+1):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pred = model(t_data)\n",
    "\n",
    "    loss = torch.mean((pred - data)**2) \n",
    "\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "\n",
    "    if (i) % num_save_plot == 0: \n",
    "\n",
    "        pred = model(t).detach()\n",
    "\n",
    "        plot_my_result(t, x, theta, t_data, data, pred)\n",
    "\n",
    "        file = \"plots/nn_%.8i.png\"%(i)\n",
    "        plt.savefig(file, bbox_inches='tight', pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
    "        files.append(file)\n",
    "\n",
    "        if (i) % episodes == 0: plt.show()\n",
    "        else: plt.close(\"all\")\n",
    "\n",
    "#save_gif_PIL(name_gif, files, fps=5, loop=0)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics Informed Neural Network\n",
    "\n",
    "> Finally, we add the underlying differential equation (\"physics loss\") to the loss function. \n",
    "\n",
    "The physics loss aims to ensure that the learned solution is consistent with the underlying differential equation. This is done by penalising the residual of the differential equation over a set of locations sampled from the domain.\n",
    "\n",
    "Here we evaluate the physics loss at 30 points uniformly spaced over the problem domain. We can calculate the derivatives of the network solution with respect to its input variable at these points using `pytorch`'s autodifferentiation features, and can then easily compute the residual of the differential equation using these quantities.\n",
    "\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://benmoseley.blog/my-research/so-what-is-a-physics-informed-neural-network/\">\n",
    "    <img src=\"https://benmoseley.blog/wp-content/uploads/2021/08/pinn.png\" \n",
    "    alt=\"Images\" width=\"600\">\n",
    "  </a>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ">One way to do this for our problem is to use a physics-informed neural network [1,2]. The idea is very simple: add the known differential equations directly into the loss function when training the neural network.\n",
    "\n",
    ">This is done by sampling a set of input training locations (\\{x_{j}\\}) and passing them through the network. Next gradients of the network’s output with respect to its input are computed at these locations (which are typically analytically available for most neural networks, and can be easily computed using autodifferentiation). Finally, the residual of the underlying differential equation is computed using these gradients, and added as an extra term in the loss function.\n",
    "\n",
    ">The physics-informed neural network is able to predict the solution far away from the experimental data points, and thus performs much better than the naive network. One could argue that this network does indeed have some concept of our prior physical principles.\n",
    "\n",
    ">The naive network is performing poorly because we are “throwing away” our existing scientific knowledge; with only the data at hand, it is like trying to understand all of the data generated by a particle collider, without having been to a physics class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import sin, cos\n",
    "\n",
    "t_physics = torch.linspace(0,t1,t1*5).view(-1,1).requires_grad_(True)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "\n",
    "lr_data = 1e-3\n",
    "lr_physics = 1e-2\n",
    "num_layers = 3\n",
    "num_hidden = 32\n",
    "\n",
    "model = FCN(1,2,num_hidden,num_layers)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr_data)\n",
    "\n",
    "\n",
    "episodes = 4000\n",
    "num_plots = 500                        \n",
    "\n",
    "\n",
    "files = []\n",
    "name_plot = 'Inverted Pendulum PINN: lr_data %s, lr_physics %s, hidden %sx%s'%(lr_data, lr_physics,num_layers, num_hidden)\n",
    "name_gif = 'PINN_lr_data_%s_lr_physics_%s_hidden_%sx%s_episodes_%s.gif'%(lr_data, lr_physics,num_layers, num_hidden, episodes)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1, episodes+1):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # normal loss\n",
    "    pred = model(t_data)\n",
    "    loss_data = torch.mean((pred - data)**2) \n",
    "\n",
    "    # physics\n",
    "    pred = model(t_physics)\n",
    "    \n",
    "    x_p = pred[:,0].view(-1,1)\n",
    "    x_d_p = torch.autograd.grad(x_p, t_physics, torch.ones_like(x_p), create_graph=True)[0]\n",
    "    x_dd_p = torch.autograd.grad(x_d_p, t_physics, torch.ones_like(x_d_p), create_graph=True)[0]\n",
    "    \n",
    "    theta_p = pred[:,1].view(-1,1)\n",
    "    theta_d_p = torch.autograd.grad(theta_p, t_physics, torch.ones_like(theta_p), create_graph=True)[0]\n",
    "    theta_dd_p = torch.autograd.grad(theta_d_p, t_physics, torch.ones_like(theta_d_p), create_graph=True)[0]\n",
    "    \n",
    "    sin_theta_p = sin(theta_p)\n",
    "    cos_theta_p = cos(theta_p)\n",
    "    \n",
    "    physics_x = M*x_dd_p + m*(l*sin_theta_p*theta_d_p**2 - l*cos_theta_p*theta_dd_p + x_dd_p)\n",
    "    physics_theta = l*m*(-g*sin_theta_p + l*theta_dd_p - cos_theta_p*x_dd_p)\n",
    "    \n",
    "    #physics = torch.cat((physics_x, physics_theta), 1)\n",
    "    #loss_physics = lr_physics*torch.mean(physics**2)\n",
    "    \n",
    "    lambda_x = 0.02\n",
    "    lambda_theta = 0.02\n",
    "    loss_physics = lambda_x * torch.mean(physics_x**2) + lambda_theta * torch.mean(physics_theta**2)\n",
    "    \n",
    "\n",
    "    #loss = loss_data + loss_physics\n",
    "    loss = loss_physics # ohne Loss durch die Messpunkte\n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    \n",
    "    # save 10 plots\n",
    "    if (i % (episodes/num_plots)) == 0:   \n",
    "        \n",
    "        pred = model(t).detach()\n",
    "        ps = t_physics.detach()\n",
    "        \n",
    "        plot_my_result(t, x, theta, t_data, data, pred, ps)\n",
    "        \n",
    "        file = \"plots/pinn_%.8i.png\"%(i)\n",
    "        plt.savefig(file, bbox_inches='tight', pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
    "        files.append(file)\n",
    "        \n",
    "        # show 5 plots\n",
    "        if (i % (episodes/5)) == 0: plt.show()\n",
    "        else: plt.close(\"all\")\n",
    "            \n",
    "\n",
    "print('duration: %s [s]' % round(time.time() - start))            \n",
    "save_gif_PIL(name_gif, files, fps=15, loop=0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepXDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Backend supported: tensorflow.compat.v1, tensorflow, pytorch\"\"\"\n",
    "import deepxde as dde\n",
    "import numpy as np\n",
    "from tensorflow import sin, cos\n",
    "\n",
    "\n",
    "def ode_system_ip(t, y):\n",
    "    \n",
    "    x, theta = y[:, 0:1], y[:, 1:]\n",
    "    \n",
    "    x_d = dde.grad.jacobian(y, t, i=0)\n",
    "    theta_d = dde.grad.jacobian(y, t, i=0)\n",
    "    \n",
    "    x_dd = dde.grad.hessian(y, t, component=0, i=0, j=0)\n",
    "    theta_dd = dde.grad.hessian(y, t, component=1, i=0, j=0)\n",
    "    \n",
    "    sin_theta = sin(theta)\n",
    "    cos_theta = cos(theta)\n",
    "        \n",
    "    #eq_x = M*x_dd+m*(l*tf.sin(theta)*theta_d**2-l*tf.cos(theta)*theta_dd+x_dd)\n",
    "    eq_x = D*x_d + M*x_dd + m*(l*sin_theta*theta_d**2-l*cos_theta*theta_dd+x_dd)\n",
    "    \n",
    "    #eq_theta = l*m*(-g*tf.sin(theta)+l*theta_dd-tf.cos(theta)*x_dd)\n",
    "    eq_theta = d*theta_d + l*m*(-g*sin_theta+l*theta_dd-cos_theta*x_dd)\n",
    "\n",
    "    return [eq_x, eq_theta]\n",
    "\n",
    "\n",
    "def boundary(_, on_initial):\n",
    "    return on_initial\n",
    "\n",
    "def func(t):\n",
    "\n",
    "    return np.hstack((x_f(t), theta_f(t)))\n",
    "\n",
    "\n",
    "geom = dde.geometry.TimeDomain(0, t1)\n",
    "ic1 = dde.IC(geom, x_f, boundary, component=0)\n",
    "ic2 = dde.IC(geom, theta_f, boundary, component=1)\n",
    "data = dde.data.PDE(geom, ode_system_ip, [ic1, ic2], num_domain=50, num_boundary=2, solution=func, num_test=100)\n",
    "\n",
    "layer_size = [1] + [50] * 3 + [2]\n",
    "activation = \"sigmoid\"\n",
    "initializer = \"Glorot uniform\"\n",
    "net = dde.maps.FNN(layer_size, activation, initializer)\n",
    "\n",
    "model = dde.Model(data, net)\n",
    "model.compile(\"adam\", lr=0.001, metrics=[\"l2 relative error\"])\n",
    "losshistory, train_state = model.train(epochs=20000)\n",
    "\n",
    "dde.saveplot(losshistory, train_state, issave=True, isplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
